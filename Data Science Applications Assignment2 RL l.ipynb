{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XchU4iAIYMIo"
      },
      "source": [
        "# **part 1 : Change the code to be modular code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XSl2TiGYMce",
        "outputId": "f2693725-c706-4a41-98a5-94eca1e22567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "#installation Libraries\n",
        "!pip install cmake 'gym[atari]' scipy\n",
        "import gym\n",
        "import sys\n",
        "from time import sleep\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#choose environment and render it\n",
        "def Set_Environment(environment_name):\n",
        "  \"\"\"Function Change_Environment\n",
        "     Input : environment_name\n",
        "     Output : change the environment name and render it as a test and return the environment\n",
        "  \"\"\"\n",
        "  env = gym.make(environment_name)\n",
        "  env.render()\n",
        "  return env"
      ],
      "metadata": {
        "id": "5zDn1RxWdnOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAS1PJ0ANOjq",
        "outputId": "ee0bec9c-5fba-413f-cd8f-0da58b0a15e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#choose environment and render it\n",
        "env=Set_Environment(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSFvozH3NQiI",
        "outputId": "c8aad26c-3301-44ad-e3a8-e97c4882b79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#change environment state for just check it\n",
        "env.s = 301\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9REqONPNSJ7",
        "outputId": "97cf16f9-3974-4d20-eb55-956ca97d9e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ],
      "source": [
        "# reset the environment environment to just choose random state and render \n",
        "env.reset() \n",
        "env.render()\n",
        "#print the environment action space and states that prints 6 action in our environment and 500 state\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqBbcQkzOoYJ"
      },
      "outputs": [],
      "source": [
        "def print_frames(frames):\n",
        "  \"\"\"\n",
        "   Function : print_frames\n",
        "   Input : frames\n",
        "   Output : print the frames \n",
        "  \"\"\"\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    #print(frame['frame'].getvalue())\n",
        "    print(frame['frame'])\n",
        "    print(f\"Timestep: {i + 1}\")\n",
        "    print(f\"State: {frame['state']}\")\n",
        "    print(f\"Action: {frame['action']}\")\n",
        "    print(f\"Reward: {frame['reward']}\")\n",
        "    sleep(.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-XV6y64Orsc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Brute_force_approach():\n",
        "  \"\"\"\n",
        "  Function Brute force approach\n",
        "  Output : return frames to print them, print Timesteps and Penalties\n",
        "  \"\"\"\n",
        "  env.s = 328  # set environment to illustration's state\n",
        "\n",
        "  epochs = 0\n",
        "  penalties, reward = 0, 0\n",
        "\n",
        "  frames = [] # for animation\n",
        "\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "      action = env.action_space.sample()\n",
        "      state, reward, done, info = env.step(action)\n",
        "\n",
        "      if reward == -10:\n",
        "          penalties += 1\n",
        "      \n",
        "      # Put each rendered frame into dict for animation\n",
        "      frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "      )\n",
        "\n",
        "      epochs += 1\n",
        "      \n",
        "      \n",
        "  print(\"Timesteps taken: {}\".format(epochs))\n",
        "  print(\"Penalties incurred: {}\".format(penalties))\n",
        "  return frames "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZGOh_mLQwom",
        "outputId": "9239f108-76d5-4901-9863-9d3a222df524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 66\n"
          ]
        }
      ],
      "source": [
        "frames = Brute_force_approach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOhq7CJKQywn",
        "outputId": "7cf75281-ed96-4276-e05b-df790879ef5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "Timestep: 200\n",
            "State: 214\n",
            "Action: 1\n",
            "Reward: -1\n"
          ]
        }
      ],
      "source": [
        "print_frames(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMA-izyNZDB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def TrainingAgentFixedHyperParametersQLearning(q_table,env,alpha,gamma,epsilon,trianingTimes):\n",
        "  \"\"\"Function Training Agent with Fixed HyperParameters\n",
        "   Input : q_table, env, alpha, gamma, epsilon and trianingTimes\n",
        "   Output : print Episode numbers that already trained and return env and qtable after training\n",
        "  \"\"\"\n",
        "  # Initialize the q table\n",
        "  \n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  for i in range(1, trianingTimes+1):\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "  print(\"Training finished.\\n\")\n",
        "  return env, q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9phI_OUNazP"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "\"\"\"Function : Evaluate agent's performance after Q-learning\n",
        "   Input : env, episodes, q_table\n",
        "   output : print episodes, timesteps and penalities and return episodes,timesteps,penalties,rewards\n",
        "\"\"\"\n",
        "def EvaluationWithQLearning(env,episodes,q_table):\n",
        "   \n",
        "  total_epochs, total_penalties = 0, 0\n",
        "  rewards = 0\n",
        "  for _ in range(episodes):\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward = 0, 0, 0\n",
        "      \n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          action = np.argmax(q_table[state])\n",
        "          state, reward, done, info = env.step(action)\n",
        "          rewards+=reward\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          epochs += 1\n",
        "\n",
        "      total_penalties += penalties\n",
        "      total_epochs += epochs\n",
        "\n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "  return episodes,(total_epochs / episodes),(total_penalties / episodes),(rewards / episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdMs2GjBP9ZC"
      },
      "outputs": [],
      "source": [
        "# Initialize the q table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4bXdkTUP_DF"
      },
      "outputs": [],
      "source": [
        "# Intialize Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD55nVK9QGXV",
        "outputId": "dad480f0-1ccf-4493-9b63-243d9edfbfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#call TrainingAgentFixedHyperParametersQLearning function with 10000 training times\n",
        "envAfterTraining,q_tableAfterTraining = TrainingAgentFixedHyperParametersQLearning(q_table,env,alpha,gamma,epsilon,100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knV5SDBDQKYk",
        "outputId": "a29ba78c-97d7-447f-9cb4-e28efaa33e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.073\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 13.073, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#evaluate our training that was with 10000 training times with 1000 episodes\n",
        "EvaluationWithQLearning(envAfterTraining,1000,q_tableAfterTraining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVbE6vLeaac-",
        "outputId": "8c8ef132-0ba8-48f2-a9ff-f798678ba850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "#change our environment to FrozenLake-v0 environment check our modularity\n",
        "env=Set_Environment(\"FrozenLake-v0\") \n",
        "\n",
        "# Intialize Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "# Initialize the q table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb3e1xuub5m2"
      },
      "outputs": [],
      "source": [
        "# frames = Brute_force_approach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i45jispbanx",
        "outputId": "6144d41e-f307-4269-ab8b-7fcab429fe36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#call TrainingAgentFixedHyperParametersQLearning function with 10000 training times\n",
        "\n",
        "envAfterTraining,q_tableAfterTraining = TrainingAgentFixedHyperParametersQLearning(q_table,env,alpha,gamma,epsilon,100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTGZXSBebb9G",
        "outputId": "118eb39f-5ec3-4362-df98-3a0e905a4370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 9.277\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 9.277, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#evaluate our training that was with 10000 training times with 1000 episodes\n",
        "EvaluationWithQLearning(envAfterTraining,1000,q_tableAfterTraining)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14O8fX0WyCU8"
      },
      "source": [
        "# **Decreasing Hyperparameters continuesly to show the affect of this decreasing **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snmuEEMal6DK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def TrainingAgentDescreasingHyperParametersQLearning(q_table,env,trianingTimes):\n",
        "  \"\"\"Function Training Agent with Descreasing HyperParameters\n",
        "   Input : q_table, epsilon and trianingTimes\n",
        "   Output : print Episode numbers that already trained and return env and qtable after training\n",
        "  \"\"\"\n",
        "  # Initialize the q table\n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  descreasing_parameters = np.linspace(0.9,0.7,1001)\n",
        "  descreasing_counter=-1;\n",
        "  for i in range(1, trianingTimes+1):\n",
        "    #decreasing hyperparameters every 100 iterations from 0.9 to 0.1\n",
        "      if i%100==0 or i==1:\n",
        "        descreasing_counter+=1\n",
        "        print(descreasing_counter)\n",
        "        alpha=descreasing_parameters[descreasing_counter]\n",
        "        gamma=descreasing_parameters[descreasing_counter]\n",
        "        epsilon=descreasing_parameters[descreasing_counter]\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "  print(\"Training finished.\\n\")\n",
        "  return env, q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld5zrTouzNr6",
        "outputId": "3a4f7595-1140-4a94-a19a-88401be260c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Change the enivronment to Taxi-v3\n",
        "env=Set_Environment(\"Taxi-v3\") \n",
        "\n",
        "# Initialize the q table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjIfiYqyydxP",
        "outputId": "cd8274ee-7b11-4afd-c5be-5f09b5cc2b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "envAfterTraining,q_tableAfterTraining = TrainingAgentDescreasingHyperParametersQLearning(q_table,env,100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaXDRv4zyftb",
        "outputId": "97206844-6f9b-42ac-d112-66c7305216b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.994\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 12.994, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "#evaluate our training that was with 10000 training times with 1000 episodes\n",
        "EvaluationWithQLearning(envAfterTraining,1000,q_tableAfterTraining)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVnn7pFR0FRM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP2Rdq9q0EMr",
        "outputId": "6accd96d-352a-4875-b58a-bf919dfad617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Change the enivronment to Taxi-v3 again and reset it \n",
        "env=Set_Environment(\"Taxi-v3\") \n",
        "\n",
        "# Initialize the q table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9GNQTBgZyTz"
      },
      "outputs": [],
      "source": [
        "def TrainingAgentFixedHyperParametersQLearningWithoutLog(q_table,env,alpha,gamma,epsilon,trianingTimes):\n",
        "  \"\"\"Function Training Agent with Fixed HyperParameters Without screen Log\n",
        "   Input : q_table, env, alpha, gamma, epsilon and trianingTimes\n",
        "   Output : print Episode numbers that already trained and return env and qtable after training\n",
        "  \"\"\"\n",
        "  # Initialize the q table\n",
        "  \n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "\n",
        "  for i in range(1, trianingTimes+1):\n",
        "      state = env.reset()\n",
        "\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "      \n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      \n",
        "\n",
        "  print(\"Training finished.\\n\")\n",
        "  return env, q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xJtWLwN0NPk",
        "outputId": "fd1a4076-123f-4403-b54a-4b794ef58079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.1 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.846\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.846 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.1 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.031\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.031 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.1 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.984\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 12.984 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.5 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.938\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 12.938 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.5 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.064\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.064 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.5 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.01\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.01 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.9 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.137\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.137 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.9 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.149\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.149 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.1 gamma 0.9 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.22\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.22 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.1 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.071\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.071 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.1 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.105\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.105 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.1 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.117\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.117 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.5 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.122\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.122 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.5 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.187\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.187 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.5 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.085\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.085 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.9 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.135\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.135 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.9 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.037\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.037 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.5 gamma 0.9 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.332\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.332 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.1 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.013\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.013 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.1 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.999\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 12.999 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.1 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.029\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.029 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.5 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.264\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.264 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.5 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.952\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 12.952 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.5 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.946\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 12.946 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.9 epsilon 0.1\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.14\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.14 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.9 epsilon 0.5\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.027\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.027 penalties 0.0\n",
            "\n",
            "***************\n",
            "at alpha 0.9 gamma 0.9 epsilon 0.9\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.098\n",
            "Average penalties per episode: 0.0\n",
            "episodes 1000 timesteps 13.098 penalties 0.0\n",
            "\n",
            "**************************\n",
            "\n",
            "Best Parameters Combinations alpha 0.1, gamma 0.5, epsilon 0.1\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 12.938\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Intialize Hyperparameters\n",
        "alpha = np.linspace(0.1,0.9,3)\n",
        "gamma = np.linspace(0.1,0.9,3)\n",
        "epsilon = np.linspace(0.1,0.9,3)\n",
        "alphaval = 0\n",
        "gammaval =0\n",
        "epsilonval=0\n",
        "episodesVal=0\n",
        "timestepsVal=0\n",
        "penaltiesVal=0\n",
        "EvaluationnValue = sys.maxsize\n",
        "for i in alpha:\n",
        "  for j in gamma:\n",
        "    for k in epsilon:\n",
        "\n",
        "      print(\"\\n***************\\nat alpha {} gamma {} epsilon {}\".format(i,j,k))\n",
        "      #call TrainingAgentFixedHyperParametersQLearning function with 10000 training times\n",
        "      envAfterTraining,q_tableAfterTraining = TrainingAgentFixedHyperParametersQLearningWithoutLog(q_table,env,i,j,j,100000)\n",
        "      #evaluate our training that was with 10000 training times with 1000 episodes\n",
        "      episodes,timesteps,penalties,rewards=EvaluationWithQLearning(envAfterTraining,1000,q_tableAfterTraining)\n",
        "      print(\"episodes {} timesteps {} penalties {}\".format(episodes,timesteps,penalties))\n",
        "      if(EvaluationnValue>(rewards/timestepsVal)):\n",
        "        EvaluationnValue = (rewards/timestepsVal)\n",
        "        alphaval = i\n",
        "        gammaval =j\n",
        "        epsilonval=k\n",
        "        episodesVal= episodes\n",
        "        penaltiesVal = penalties\n",
        "        timestepsVal= timesteps\n",
        "print(\"\\n**************************\\n\")    \n",
        "print(\"Best Parameters Combinations alpha {}, gamma {}, epsilon {}\".format(alphaval,gammaval,epsilonval))        \n",
        "print(f\"Results after {episodesVal} episodes:\")\n",
        "print(f\"Average timesteps per episode: {timestepsVal}\")\n",
        "print(f\"Average penalties per episode: {penaltiesVal}\")\n",
        "print(f\"Average rewards per episode: {rewards}\")\n",
        "print(f\"Overall Evaluation: {rewards/timestepsVal}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}